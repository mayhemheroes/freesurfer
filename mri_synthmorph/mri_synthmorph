#!/usr/bin/env python3

# SynthMorph registration script using TensorFlow. To be used for affine and
# deformable registration and distributed with Docker.

import os
import sys
import time
import argparse
import numpy as np
import nibabel as nib
import tensorflow as tf
import neurite as ne
import voxelmorph as vxm

# Defaults.
in_shape = (256, 256, 256)

if os.getenv("FREESURFER_HOME") is not None:
   # use freesurfer install paths
   base_path = os.environ['FREESURFER_HOME'] + '/' + 'models'
   weights = {
       'rigid': base_path + '/' + 'mu40-crazy/seg_feat_neck_rigid_3d_256f/10000.h5',
       'affine': base_path + '/' + 'mu40-crazy/seg_feat_neck_noise_3_3d_256f/10000.h5',
       'deform': base_path + '/' + 'mu40-crazy/weights_def_half.h5',
   }
else:
   # use your own local paths
   weights = {
       'rigid': '/autofs/cluster/fssubjects/test/mu40-crazy/models/seg_feat_neck_rigid_3d_256f/10000.h5',
       'affine': '/autofs/cluster/fssubjects/test/mu40-crazy/models/seg_feat_neck_noise_3_3d_256f/10000.h5',
       'deform': '/autofs/cluster/fssubjects/test/mu40-crazy/weights_def_half.h5',
   }


# References.
ref = '''If our work is useful to you, please consider citing:
        M Hoffmann, B Billot, DN Greve, JE Iglesias, B Fischl, AV Dalca.
        SynthMorph: learning contrast-invariant registration without acquired
        images. IEEE Transactions on Medical Imaging (TMI), 41 (3), 543-558,
        2022. https://doi.org/10.1109/TMI.2021.3116879
'''

desc = f'''Register unprocessed brain images with SynthMorph.'''

prog = os.path.basename(sys.argv[0])
epilog = f'''
IMAGE FORMAT
    The registration network expects input images of shape {in_shape} with
    isotropic 1-mm voxels min-max normalized into the interval [0, 1]. It also
    assumes the axes of the image-data array have approximate LIA orientation
    (left-inferior-anterior).

    These requirements will be handled internally, and need a correctly set
    image-to-world matrix as part of the image header. Accepted image formats
    are those supported by NiBabel and include: MGH format (.mgz) and NIfTI
    (.nii.gz, .nii).
    
    
TRANSFORMS
    The output transforms of this script are defined in RAS space, transforming
    world coordinates of the fixed image to the moving image. Likewise, we will
    assume that any linear transform passed to initialze the registration
    operates in RAS space. We save linear transforms as a 4-by-4 matrix in text
    format and non-linear displacements fields as an image. See IMAGE FORMAT.

    For converting, concatenating, and applying transforms to other images,
    consider the FreeSurfer tools: lta_convert/mri_warp_convert,
    mri_concatenate_lta/mri_concatenate_gcam, and mri_convert, respectively.
    
    
EXAMPLES
    Affine registration:
        {prog} --affine --trans affine.txt moving.mgz target.mgz

    Affine registration updating the moved image header:
        {prog} --affine --header-only --moved moved.mgz moving.mgz target.mgz

    Initialize deformable registration with an affine transform:
        {prog} --init affine.txt --trans deform.nii.gz moving.mgz target.mgz

    Apply a linear transform to another image using FreeSurfer tools:
        lta_convert --src moving.mgz --trg fixed.nii.gz
            --inras affine.txt --outlta affine.lta
        mri_convert -at affine.lta in.mgz out.mgz

    Apply a deformable transform to another image using FreeSurfer tools:
        mri_warp_convert -g --inras deform.nii.gz --outm3z deform.m3z
        mri_convert -at deform.m3z in.nii.gz out.nii.gz
    
    
CONTACT
    Report bugs to <freesurfer@nmr.mgh.harvard.edu>.
    
    
REFERENCES
    {ref}
'''


class indent_formatter(argparse.HelpFormatter):
    def __init__(self, **kwargs):
        super().__init__(indent_increment=4, **kwargs)
        self._width += 2
        self._max_help_position = 2 * self._indent_increment

    def _split_lines(self, text, width):
        lines = super()._split_lines(text, width)
        return (*lines, '')

    def _format_usage(self, *args):
        return super()._format_usage(*args[:-1], 'USAGE: ')


# Command-line arguments.
bases = (
    argparse.RawDescriptionHelpFormatter,
    indent_formatter,
)
p = argparse.ArgumentParser(
    formatter_class=type('formatter', bases, {}),
    description=desc,
    add_help=False,
    epilog=epilog,
)
p.add_argument('source', metavar='SOURCE', type=str, help='''
    Input source image. See IMAGE FORMAT.
''')
p.add_argument('target', metavar='TARGET', type=str, help='''
    Input target image. See IMAGE FORMAT.
''')
p.add_argument('--moved', type=str, help='''
    Output moved image path. See IMAGE FORMAT.
''')
p.add_argument('--trans', type=str, help='''
    Output transform path. Save a text file for linear or an image file for
    deformable registration. Will include any initialization. See TRANSFORMS.
''')
p.add_argument('--header-only', action='store_true', help='''
    Update the header of moved output image instead of resampling. Applies to
    linear registration only.
''')
p.add_argument('-a', '--affine', action='store_true', help='''
    Predict affine instead of deformable transform.
''')
p.add_argument('-r', '--rigid', action='store_true', help='''
    Predict rigid instead of deformable transform (experimental).
''')
p.add_argument('-i', '--init', type=str, help='''
    Linear transform to initialize with. See TRANSFORMS.
''')
p.add_argument('-t', '--threads', type=int, help='''
    Number of parallel threads. Defaults to the number of CPU cores.
''')
p.add_argument('-g', '--gpu', action='store_true', help='''Use the GPU.''')
p.add_argument('--model', type=str, help=''' Alternative model weights.''')
p.add_argument('-h', '--help', action='help', help='''
    Display this help text and exit.
''')
p._positionals.title = 'POSITIONAL ARGUMENTS'
p._optionals.title = 'OPTIONAL ARGUMENTS'
arg = p.parse_args()

# Todo:
# [ ] Fail on multi-frame images.
# [ ] Support single-slice images.
# [ ] Compare index matrix in model vs later.
# [ ] Refactor.
# [ ] Optimize regularization.


def save(path, dat, affine, dtype=None):
    # Usi NiBabel's caching functionality to avoid re-reading from disk.
    if isinstance(dat, nib.filebasedimages.FileBasedImage):
        if dtype is None:
            dtype = dat.dataobj.dtype
        dat = dat.get_fdata(dtype=np.float32)

    dat = np.squeeze(dat)
    dat = np.asarray(dat, dtype)

    # Avoid warning about missing units when reading with FS.
    out = nib.Nifti1Image(dat, affine)
    out.header.set_xyzt_units(xyz='mm', t='sec')
    nib.save(out, filename=path)


def ori_to_ori(old, new='LIA', old_shape=None, zero_center=False):
    '''Construct matrix transforming coordinates from a voxel space with a new
    predominant anatomical axis orientation to an old orientation, by swapping
    and flipping axes. Operates in zero-based index space unless the space is
    to be zero-centered. The old shape must be specified if the old image is
    not a NiBabel object.'''
    def extract_ori(x):
        if isinstance(x, nib.filebasedimages.FileBasedImage):
            x = x.affine
        if isinstance(x, np.ndarray):
            return nib.orientations.io_orientation(x)
        if isinstance(x, str):
            return nib.orientations.axcodes2ornt(x)

    # Old shape.
    if zero_center:
        old_shape = (1, 1, 1)
    if old_shape is None:
        old_shape = old.shape

    # Transform from new to old index coordinates.
    old = extract_ori(old)
    new = extract_ori(new)
    new_to_old = nib.orientations.ornt_transform(old, new)
    return nib.orientations.inv_ornt_aff(new_to_old, old_shape)


def net_to_vox(im, out_shape=in_shape):
    '''Construct coordinate transform from isotropic 1-mm voxel space with
    gross LIA orentiation centered on the FOV - to the original image index
    space. The target space is a scaled and shifted voxel space, not world
    space.'''
    if isinstance(im, str):
        im = nib.load(im)

    # Gross LIA to predominant anatomical orientation of input image.
    assert isinstance(im, nib.filebasedimages.FileBasedImage) 
    lia_to_ori = ori_to_ori(im, new='LIA', old_shape=out_shape)

    # Scaling from millimeter to input voxels.
    vox_size = np.sqrt(np.sum(im.affine[:-1, :-1] ** 2, axis=0))
    scale = np.diag((*1 / vox_size, 1))

    # Shift from cen
    shift = np.eye(4)
    shift[:-1, -1] = 0.5 * (im.shape - out_shape / vox_size)

    # Total transform.
    return shift @ scale @ lia_to_ori


def transform(im, trans, shape=in_shape, normalize=False):
    '''Apply transformation matrix or field operating in zero-based index space
    to an image.'''
    if isinstance(im, nib.filebasedimages.FileBasedImage):
        im = im.get_fdata(dtype=np.float32)

    # Add singleton feature dimension if needed.
    if tf.rank(im) == 3:
        im = im[..., tf.newaxis]

    # Remove last row of matrix transforms.
    if tf.rank(trans) == 2 and trans.shape[0] == trans.shape[1]:
        trans = trans[:-1, :]

    out = vxm.utils.transform(
        im, trans, fill_value=0, shift_center=False, shape=shape,
    )

    if normalize:
        out -= tf.reduce_min(out)
        out /= tf.reduce_max(out)
    return out[tf.newaxis, ...]


def vm_key_weighted(
    in_shape=None,
    in_model=None,
    num_key=64,
    enc_nf=[256] * 4,
    dec_nf=[256] * 0,
    add_nf=[256] * 4,
    half_res=True,
    rigid=False,
):
    '''Find landmarks in images using a single-image detector and fit affine
    transform. Take mean as translation instead of fitting, and fit the
    transform in a centered frame.'''
    # Inputs.
    if in_model is None:
        source = tf.keras.Input(shape=(*in_shape, 1))
        target = tf.keras.Input(shape=(*in_shape, 1))
        in_model = tf.keras.Model(*[(source, target)] * 2)
    source, target = in_model.outputs[:2]

    in_shape = np.asarray(source.shape[1:-1])
    num_dim = len(in_shape)
    assert num_dim in (2, 3), 'only 2D and 3D supported'

    # Layers.
    down = getattr(tf.keras.layers, f'MaxPool{num_dim}D')()
    up = getattr(tf.keras.layers, f'UpSampling{num_dim}D')()
    act = tf.keras.layers.LeakyReLU(0.2)
    conv = getattr(tf.keras.layers, f'Conv{num_dim}D')
    prop = dict(kernel_size=3, padding='same')

    # Internal U-Net.
    inp = tf.keras.Input(shape=(*in_shape, 1))
    x = down(inp) if half_res else inp

    # Encoder.
    enc = []
    for n in enc_nf:
        x = conv(n, **prop)(x)
        x = act(x)
        enc.append(x)
        x = down(x)

    # Decoder.
    for n in dec_nf:
        x = conv(n, **prop)(x)
        x = act(x)
        x = tf.keras.layers.concatenate([up(x), enc.pop()])

    # Additional convolutions.
    for n in add_nf:
        x = conv(n, **prop)(x)
        x = act(x)

    # Features.
    x = conv(num_key, activation='relu', **prop)(x)
    net = tf.keras.Model(inp, outputs=x)
    key_1 = net(source)
    key_2 = net(target)

    # Barycenters.
    prop = dict(axes=range(1, num_dim + 1), normalize=True, shift_center=True)
    cen_1 = ne.utils.barycenter(key_1, **prop) * in_shape
    cen_2 = ne.utils.barycenter(key_2, **prop) * in_shape
    
    # Weights.
    axes = range(1, num_dim + 1)
    pow_1 = tf.reduce_sum(key_1, axis=axes)
    pow_2 = tf.reduce_sum(key_2, axis=axes)
    pow_1 /= tf.reduce_sum(pow_1, axis=-1, keepdims=True)
    pow_2 /= tf.reduce_sum(pow_2, axis=-1, keepdims=True)
    weights = pow_1 * pow_2

    # Least squares.
    out = vxm.utils.fit_affine(cen_1, cen_2, weights=weights)
    if rigid:
        out = vxm.utils.affine_matrix_to_params(out)
        out = out[:, :num_dim * (num_dim + 1) // 2]
        out = vxm.layers.ParamsToAffineMatrix(ndims=num_dim)(out)

    return tf.keras.Model(in_model.inputs, out)


def vm_dense(
    in_shape=None,
    input_model=None,
    enc_nf=[256] * 4,
    dec_nf=[256] * 4,
    add_nf=[256] * 4,
    int_steps=5,
    upsample=True,
    half_res=True,
):
    '''Deformable registration network.'''
    if input_model is None:
        source = tf.keras.Input(shape=(*in_shape, 1))
        target = tf.keras.Input(shape=(*in_shape, 1))
        input_model = tf.keras.Model(*[(source, target)] * 2)
    source, target = input_model.outputs[:2]

    in_shape = np.asarray(source.shape[1:-1])
    num_dim = len(in_shape)
    assert num_dim in (2, 3), 'only 2D and 3D supported'

    down = getattr(tf.keras.layers, f'MaxPool{num_dim}D')()
    up = getattr(tf.keras.layers, f'UpSampling{num_dim}D')()
    act = tf.keras.layers.LeakyReLU(0.2)
    conv = getattr(tf.keras.layers, f'Conv{num_dim}D')
    prop = dict(kernel_size=3, padding='same')

    # Encoder.
    x = tf.keras.layers.concatenate((source, target))
    if half_res:
        x = down(x)
    enc = [x]
    for n in enc_nf:
        x = conv(n, **prop)(x)
        x = act(x)
        enc.append(x)
        x = down(x)

    # Decoder.
    for n in dec_nf:
        x = conv(n, **prop)(x)
        x = act(x)
        x = tf.keras.layers.concatenate([up(x), enc.pop()])

    # Additional convolutions.
    for n in add_nf:
        x = conv(n, **prop)(x)
        x = act(x)

    # Transform.
    x = conv(num_dim, **prop)(x)
    if int_steps > 0:
        x = vxm.layers.VecInt(method='ss', int_steps=int_steps)(x)

    # Rescaling.
    zoom = source.shape[1] // x.shape[1]
    if upsample and zoom > 1:
        x = vxm.layers.RescaleTransform(zoom)(x)

    return tf.keras.Model(input_model.inputs, outputs=x)


# Setup.
os.environ['CUDA_VISIBLE_DEVICES'] = '0' if arg.gpu else ''
if arg.threads:
    tf.config.threading.set_inter_op_parallelism_threads(arg.threads)
    tf.config.threading.set_intra_op_parallelism_threads(arg.threads)


# Input data.
mov = nib.load(arg.source)
fix = nib.load(arg.target)


# Coordinate transforms. We will need these to take the images from their
# native voxel spaces to network space. Voxel and network spaces are different
# for each image. Network space is an isotropic 1-mm space centered on the
# original image. Its axes are aligned with the original voxel data but flipped
# and swapped to gross LIA orientation, which the network will expect.
net_to_mov = net_to_vox(mov)
net_to_fix = net_to_vox(fix)
mov_to_net = np.linalg.inv(net_to_mov)
fix_to_net = np.linalg.inv(net_to_fix)

# Transforms from and to world space (RAS). There is only one world.
mov_to_ras = mov.affine
fix_to_ras = fix.affine
ras_to_mov = np.linalg.inv(mov_to_ras)
ras_to_fix = np.linalg.inv(fix_to_ras)

# Transforms between zero-centered and zero-based voxel spaces.
ind_to_cen = np.eye(4)
ind_to_cen[:-1, -1] = -0.5 * (np.asarray(in_shape) - 1)
cen_to_ind = np.eye(4)
cen_to_ind[:-1, -1] = +0.5 * (np.asarray(in_shape) - 1)

# Incorporate an initial linear transform operating in RAS. It goes from fixed
# to moving coordinates, so we start with fixed network space on the right.
if arg.init:
    aff = np.loadtxt(arg.init)
    net_to_mov = ras_to_mov @ aff @ fix_to_ras @ net_to_fix


# Take the input images to network space. When saving the moving image with the
# correct voxel-to-RAS matrix after incorporating an initial linear transform,
# an image viewer taking this matrix into account will show an unchanged image.
# However, the network only sees the voxel data, which have been moved.
inputs = (
    transform(mov, net_to_mov, shape=in_shape, normalize=True),
    transform(fix, net_to_fix, shape=in_shape, normalize=True),
)
#save('0.nii.gz', dat=inputs[0], affine=mov.affine @ net_to_mov)
#save('1.nii.gz', dat=inputs[1], affine=fix.affine @ net_to_fix)


# Model.
assert not (arg.affine and arg.rigid), '--affine, --rigid mutually exclusive'
is_linear = arg.affine or arg.rigid
if is_linear:
    model = vm_key_weighted(in_shape, rigid=arg.rigid)

else:
    model = vm_dense(in_shape)

if not arg.model:
    if arg.rigid:
        arg.model = weights['rigid']
    elif arg.affine:
        arg.model = weights['affine']
    else:
        arg.model = weights['deform']
model.load_weights(arg.model)
trans = model(inputs)


# Add the last row to create a full matrix. Convert from zero-centered to
# zero-based indices. Then compute the transform from native fixed to native
# moving voxel spaces. Also compute a transform operating in RAS.
if is_linear:
    trans = np.concatenate((
        np.squeeze(trans),
        np.reshape((0, 0, 0, 1), newshape=(1, -1)),
    ))
    trans = cen_to_ind @ trans @ ind_to_cen
    trans_vox = net_to_mov @ trans @ fix_to_net
    trans_ras = mov_to_ras @ trans_vox @ ras_to_fix

else:
    # Construct grid of zero-based index coordinates and shape (3, N) in native
    # fixed voxel space, where N is the number of voxels.
    x_fix = (tf.range(x, dtype=tf.float32) for x in fix.shape)
    x_fix = tf.meshgrid(*x_fix, indexing='ij')
    x_fix = tf.stack(x_fix)
    x_fix = tf.reshape(x_fix, shape=(3, -1))

    # Transform fixed voxel coordinates to the fixed network space.
    x_out = fix_to_net[:-1, -1:] + (fix_to_net[:-1, :-1] @ x_fix)
    x_out = tf.transpose(x_out)

    # Add predicted warp to coordinates to go to the moving network space.
    trans = tf.squeeze(trans)
    x_out += ne.utils.interpn(trans, x_out, fill_value=0)
    x_out = tf.transpose(x_out)

    # Transform coordinates to the native moving voxel space. Subtract fixed
    # coordinates to obtain displacement from fixed to moving voxel space.
    x_out = net_to_mov[:-1, -1:] + (net_to_mov[:-1, :-1] @ x_out)
    trans_vox = tf.transpose(x_out - x_fix)
    trans_vox = tf.reshape(trans_vox, shape=(*fix.shape, -1))

    # Displacement from fixed to moving RAS coordinates.
    x_ras = fix_to_ras[:-1, -1:] + (fix_to_ras[:-1, :-1] @ x_fix)
    x_out = mov_to_ras[:-1, -1:] + (mov_to_ras[:-1, :-1] @ x_out)
    trans_ras = tf.transpose(x_out - x_ras)
    trans_ras = tf.reshape(trans_ras, shape=(*fix.shape, -1))


# Output transforms operating in RAS.
if arg.trans:
    if is_linear:
        np.savetxt(fname=arg.trans, X=trans_ras, fmt='%.8f %.8f %.8f %.8f')
    else:
        save(arg.trans, dat=trans_ras, affine=fix.affine)


# Output moved image. Keep same data type as the input.
if arg.moved:
    if arg.header_only:
        assert is_linear, '--header-only requires --affine or --rigid'
        mov_to_fix = np.linalg.inv(trans_ras)
        save(arg.moved, dat=mov, affine=mov_to_fix @ mov.affine)
    else:
        out = transform(mov, trans=trans_vox, shape=fix.shape)
        save(arg.moved, dat=out, affine=fix.affine, dtype=mov.dataobj.dtype)


print(f'\nThank you for using SynthMorph. {ref}')
